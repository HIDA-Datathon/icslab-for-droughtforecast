{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0,'/p/project/training2005/jupyter/kernels/tensorflow_test/lib/python3.6/site-packages')\n",
    "# import tensorflow\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from keras import layers, Input, Model\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ttd = np.load('redttd.npy')\n",
    "ptd = np.load('redptd.npy')\n",
    "ntd = np.load('/p/project/training2005/HZG_Challenge/nao_index_train.npy')\n",
    "\n",
    "ttst = np.load('redttst.npy')\n",
    "ptst = np.load('redptst.npy')\n",
    "\n",
    "#plt.plot(range(data.shape[1]),data[0])\n",
    "\n",
    "def _add_layer(dim, input_tensor, activ):\n",
    "    if activ != '':\n",
    "        x = layers.Dense(dim, activation=activ)(input_tensor)\n",
    "    else: \n",
    "        x = layers.Dense(dim)(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def dense_cell(dim, input_tensor, activs):\n",
    "    x = _add_layer(dim, input_tensor, activs[0])\n",
    "    for a in activs[1:]:\n",
    "        x = _add_layer(dim,x,a)\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "def cnn_cell(x, ks, st, filters):\n",
    "    x_shortcut = x\n",
    "    x = layers.Conv1D(filters=filters[0], kernel_size=1, strides=st, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(filters=filters[1], kernel_size=ks, strides=1, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(filters=filters[2], kernel_size=1, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(filters=1, kernel_size=1, strides=1, padding='same')(x)\n",
    "#     x = layers.UpSampling1D(size=5)(x)\n",
    "    x = layers.Add()([x,x_shortcut])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def build_model():\n",
    "    inp1 = Input(shape=(5,1))\n",
    "    inp2 = Input(shape=(5,1))\n",
    "    #out = layers.Dense(1)(inp)\n",
    "    \n",
    "    #first branch\n",
    "    cx = cnn_cell(inp1, 2, 1, [32,32,32])\n",
    "    x = dense_cell(100, cx, ['relu']*10 + [''])\n",
    "    \n",
    "    #second branch\n",
    "    cy = cnn_cell(inp2, 2, 1, [32,32,32])\n",
    "    y = dense_cell(100, cy, ['relu']*10 + [''])\n",
    "    \n",
    "    #merge branches\n",
    "    out = layers.Add()([x,y])\n",
    "    out = layers.Flatten()(out)\n",
    "    out = layers.Dense(1)(out)\n",
    "    model = Model([inp1,inp2], out)\n",
    "  \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def train_model(model, params ={'vsplit':0.111,'ne':145,'bs':256}):\n",
    "#     d1 = \n",
    "    history = model.fit([np.reshape(ttd,(ttd.shape[0],ttd.shape[1],1)),\n",
    "                         np.reshape(ptd,(ptd.shape[0],ptd.shape[1],1))], \n",
    "                        np.reshape(ntd,(ntd.shape[0],1)),\n",
    "                        epochs=params['ne'], \n",
    "                        batch_size=params['bs'],\n",
    "                        validation_split=params['vsplit'])\n",
    "    return [model, history]\n",
    "\n",
    "def test_model(model):\n",
    "    return model.predict([ttst,ptst])\n",
    "\n",
    "def plot_losses(epochs, history):\n",
    "    #ks = history.keys()\n",
    "    ks =['loss','val_loss','mean_absolute_error','val_mean_absolute_error']\n",
    "    [plt.plot(range(epochs), history[k],'x-') for k in ks]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 5, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 5, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5, 32)        64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 5, 32)        64          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 5, 32)        128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 5, 32)        128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 5, 32)        2080        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 5, 32)        2080        batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 5, 32)        128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 5, 32)        128         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 5, 32)        1056        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 5, 32)        1056        batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 5, 32)        128         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 5, 32)        128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5, 1)         33          batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5, 1)         33          batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 5, 1)         0           conv1d_4[0][0]                   \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 5, 1)         0           conv1d_8[0][0]                   \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 5, 1)         4           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 5, 1)         4           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5, 100)       200         batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 5, 100)       200         batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 5, 100)       400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 5, 100)       400         dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5, 100)       10100       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 5, 100)       10100       batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 5, 100)       400         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 5, 100)       400         dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 5, 100)       10100       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 5, 100)       10100       batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 5, 100)       400         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 5, 100)       400         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 5, 100)       10100       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 5, 100)       10100       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 5, 100)       400         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 5, 100)       400         dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 5, 100)       10100       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 5, 100)       10100       batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 5, 100)       400         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 5, 100)       400         dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 5, 100)       10100       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 5, 100)       10100       batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 5, 100)       400         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 5, 100)       400         dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 5, 100)       10100       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 5, 100)       10100       batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 5, 100)       400         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 5, 100)       400         dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 5, 100)       10100       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 5, 100)       10100       batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 5, 100)       400         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 5, 100)       400         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 5, 100)       10100       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 5, 100)       10100       batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 5, 100)       400         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 5, 100)       400         dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 5, 100)       10100       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 5, 100)       10100       batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 5, 100)       400         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 5, 100)       400         dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 5, 100)       10100       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 5, 100)       10100       batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 5, 100)       400         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 5, 100)       400         dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 5, 100)       0           batch_normalization_15[0][0]     \n",
      "                                                                 batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 500)          0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1)            501         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 218,943\n",
      "Trainable params: 214,155\n",
      "Non-trainable params: 4,788\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 800 samples, validate on 100 samples\n",
      "Epoch 1/145\n",
      "800/800 [==============================] - 15s 18ms/step - loss: 3.7251 - mean_absolute_error: 1.5404 - val_loss: 58.2402 - val_mean_absolute_error: 6.5469\n",
      "Epoch 2/145\n",
      "800/800 [==============================] - 0s 178us/step - loss: 2.9837 - mean_absolute_error: 1.3707 - val_loss: 30.6271 - val_mean_absolute_error: 4.4905\n",
      "Epoch 3/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 2.0172 - mean_absolute_error: 1.1309 - val_loss: 9.2618 - val_mean_absolute_error: 2.4572\n",
      "Epoch 4/145\n",
      "800/800 [==============================] - 0s 177us/step - loss: 1.9420 - mean_absolute_error: 1.1271 - val_loss: 8.4156 - val_mean_absolute_error: 2.3013\n",
      "Epoch 5/145\n",
      "800/800 [==============================] - 0s 178us/step - loss: 1.6476 - mean_absolute_error: 1.0182 - val_loss: 5.6979 - val_mean_absolute_error: 1.8683\n",
      "Epoch 6/145\n",
      "800/800 [==============================] - 0s 178us/step - loss: 1.4681 - mean_absolute_error: 0.9614 - val_loss: 5.7479 - val_mean_absolute_error: 1.9016\n",
      "Epoch 7/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 1.5287 - mean_absolute_error: 0.9907 - val_loss: 4.8872 - val_mean_absolute_error: 1.7517\n",
      "Epoch 8/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 1.4815 - mean_absolute_error: 0.9537 - val_loss: 3.4586 - val_mean_absolute_error: 1.5297\n",
      "Epoch 9/145\n",
      "800/800 [==============================] - 0s 177us/step - loss: 1.4134 - mean_absolute_error: 0.9399 - val_loss: 1.9916 - val_mean_absolute_error: 1.1609\n",
      "Epoch 10/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 1.3276 - mean_absolute_error: 0.9169 - val_loss: 2.4471 - val_mean_absolute_error: 1.2153\n",
      "Epoch 11/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 1.2314 - mean_absolute_error: 0.8902 - val_loss: 2.1501 - val_mean_absolute_error: 1.1448\n",
      "Epoch 12/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 1.1155 - mean_absolute_error: 0.8369 - val_loss: 2.0592 - val_mean_absolute_error: 1.1616\n",
      "Epoch 13/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 1.0964 - mean_absolute_error: 0.8352 - val_loss: 2.0844 - val_mean_absolute_error: 1.1404\n",
      "Epoch 14/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 1.0734 - mean_absolute_error: 0.8251 - val_loss: 2.0008 - val_mean_absolute_error: 1.1083\n",
      "Epoch 15/145\n",
      "800/800 [==============================] - 0s 178us/step - loss: 1.0198 - mean_absolute_error: 0.7975 - val_loss: 1.6548 - val_mean_absolute_error: 1.0175\n",
      "Epoch 16/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 1.0632 - mean_absolute_error: 0.8229 - val_loss: 2.0293 - val_mean_absolute_error: 1.1759\n",
      "Epoch 17/145\n",
      "800/800 [==============================] - 0s 177us/step - loss: 1.0621 - mean_absolute_error: 0.8262 - val_loss: 2.5025 - val_mean_absolute_error: 1.3004\n",
      "Epoch 18/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 1.0607 - mean_absolute_error: 0.8285 - val_loss: 2.2085 - val_mean_absolute_error: 1.1921\n",
      "Epoch 19/145\n",
      "800/800 [==============================] - 0s 179us/step - loss: 1.0857 - mean_absolute_error: 0.8405 - val_loss: 1.8881 - val_mean_absolute_error: 1.1265\n",
      "Epoch 20/145\n",
      "800/800 [==============================] - 0s 177us/step - loss: 1.0768 - mean_absolute_error: 0.8282 - val_loss: 1.7600 - val_mean_absolute_error: 1.0989\n",
      "Epoch 21/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 1.0832 - mean_absolute_error: 0.8299 - val_loss: 1.6014 - val_mean_absolute_error: 1.0541\n",
      "Epoch 22/145\n",
      "800/800 [==============================] - 0s 179us/step - loss: 1.0821 - mean_absolute_error: 0.8322 - val_loss: 1.8251 - val_mean_absolute_error: 1.1052\n",
      "Epoch 23/145\n",
      "800/800 [==============================] - 0s 177us/step - loss: 0.9796 - mean_absolute_error: 0.7944 - val_loss: 2.1943 - val_mean_absolute_error: 1.2385\n",
      "Epoch 24/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 1.0375 - mean_absolute_error: 0.7978 - val_loss: 1.8493 - val_mean_absolute_error: 1.1381\n",
      "Epoch 25/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 1.0165 - mean_absolute_error: 0.8035 - val_loss: 1.9862 - val_mean_absolute_error: 1.1892\n",
      "Epoch 26/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 0.9861 - mean_absolute_error: 0.7856 - val_loss: 1.9643 - val_mean_absolute_error: 1.1758\n",
      "Epoch 27/145\n",
      "800/800 [==============================] - 0s 182us/step - loss: 0.9966 - mean_absolute_error: 0.7883 - val_loss: 1.9233 - val_mean_absolute_error: 1.1293\n",
      "Epoch 28/145\n",
      "800/800 [==============================] - 0s 174us/step - loss: 1.0469 - mean_absolute_error: 0.8284 - val_loss: 1.9262 - val_mean_absolute_error: 1.1661\n",
      "Epoch 29/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 1.2202 - mean_absolute_error: 0.8656 - val_loss: 2.2425 - val_mean_absolute_error: 1.1888\n",
      "Epoch 30/145\n",
      "800/800 [==============================] - 0s 177us/step - loss: 1.1763 - mean_absolute_error: 0.8617 - val_loss: 1.6309 - val_mean_absolute_error: 1.0647\n",
      "Epoch 31/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 1.1479 - mean_absolute_error: 0.8319 - val_loss: 1.7720 - val_mean_absolute_error: 1.1026\n",
      "Epoch 32/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 1.0119 - mean_absolute_error: 0.8034 - val_loss: 1.9643 - val_mean_absolute_error: 1.1656\n",
      "Epoch 33/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 0.9762 - mean_absolute_error: 0.7810 - val_loss: 1.8647 - val_mean_absolute_error: 1.1179\n",
      "Epoch 34/145\n",
      "800/800 [==============================] - 0s 177us/step - loss: 0.9888 - mean_absolute_error: 0.7933 - val_loss: 1.8969 - val_mean_absolute_error: 1.1379\n",
      "Epoch 35/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 0.9797 - mean_absolute_error: 0.7804 - val_loss: 1.9556 - val_mean_absolute_error: 1.1448\n",
      "Epoch 36/145\n",
      "800/800 [==============================] - 0s 183us/step - loss: 0.9866 - mean_absolute_error: 0.7926 - val_loss: 2.0089 - val_mean_absolute_error: 1.1213\n",
      "Epoch 37/145\n",
      "800/800 [==============================] - 0s 187us/step - loss: 1.0067 - mean_absolute_error: 0.7936 - val_loss: 2.2467 - val_mean_absolute_error: 1.2214\n",
      "Epoch 38/145\n",
      "800/800 [==============================] - 0s 197us/step - loss: 1.0086 - mean_absolute_error: 0.8046 - val_loss: 2.6756 - val_mean_absolute_error: 1.3214\n",
      "Epoch 39/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 1.0134 - mean_absolute_error: 0.7923 - val_loss: 2.2847 - val_mean_absolute_error: 1.2328\n",
      "Epoch 40/145\n",
      "800/800 [==============================] - 0s 179us/step - loss: 1.0045 - mean_absolute_error: 0.7885 - val_loss: 2.1699 - val_mean_absolute_error: 1.2138\n",
      "Epoch 41/145\n",
      "800/800 [==============================] - 0s 177us/step - loss: 0.9373 - mean_absolute_error: 0.7661 - val_loss: 2.1736 - val_mean_absolute_error: 1.2236\n",
      "Epoch 42/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 0.9547 - mean_absolute_error: 0.7761 - val_loss: 2.2594 - val_mean_absolute_error: 1.2323\n",
      "Epoch 43/145\n",
      "800/800 [==============================] - 0s 178us/step - loss: 0.9323 - mean_absolute_error: 0.7623 - val_loss: 2.2766 - val_mean_absolute_error: 1.2080\n",
      "Epoch 44/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 0.9451 - mean_absolute_error: 0.7783 - val_loss: 1.7742 - val_mean_absolute_error: 1.0623\n",
      "Epoch 45/145\n",
      "800/800 [==============================] - 0s 179us/step - loss: 0.9152 - mean_absolute_error: 0.7486 - val_loss: 1.6185 - val_mean_absolute_error: 1.0398\n",
      "Epoch 46/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 0.9199 - mean_absolute_error: 0.7712 - val_loss: 1.8767 - val_mean_absolute_error: 1.1026\n",
      "Epoch 47/145\n",
      "800/800 [==============================] - 0s 185us/step - loss: 0.9572 - mean_absolute_error: 0.7775 - val_loss: 1.7614 - val_mean_absolute_error: 1.0771\n",
      "Epoch 48/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 1.0309 - mean_absolute_error: 0.8066 - val_loss: 1.5597 - val_mean_absolute_error: 1.0278\n",
      "Epoch 49/145\n",
      "800/800 [==============================] - 0s 179us/step - loss: 0.9333 - mean_absolute_error: 0.7664 - val_loss: 1.4797 - val_mean_absolute_error: 0.9816\n",
      "Epoch 50/145\n",
      "800/800 [==============================] - 0s 181us/step - loss: 0.9330 - mean_absolute_error: 0.7655 - val_loss: 1.5038 - val_mean_absolute_error: 0.9985\n",
      "Epoch 51/145\n",
      "800/800 [==============================] - 0s 181us/step - loss: 0.9856 - mean_absolute_error: 0.7920 - val_loss: 1.7018 - val_mean_absolute_error: 1.0752\n",
      "Epoch 52/145\n",
      "800/800 [==============================] - 0s 174us/step - loss: 0.9867 - mean_absolute_error: 0.7868 - val_loss: 1.7560 - val_mean_absolute_error: 1.0775\n",
      "Epoch 53/145\n",
      "800/800 [==============================] - 0s 174us/step - loss: 1.0064 - mean_absolute_error: 0.7968 - val_loss: 1.5845 - val_mean_absolute_error: 1.0277\n",
      "Epoch 54/145\n",
      "800/800 [==============================] - 0s 177us/step - loss: 0.9575 - mean_absolute_error: 0.7675 - val_loss: 1.6629 - val_mean_absolute_error: 1.0775\n",
      "Epoch 55/145\n",
      "800/800 [==============================] - 0s 159us/step - loss: 0.9983 - mean_absolute_error: 0.7902 - val_loss: 1.6757 - val_mean_absolute_error: 1.0811\n",
      "Epoch 56/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 1.0406 - mean_absolute_error: 0.8016 - val_loss: 1.5118 - val_mean_absolute_error: 1.0185\n",
      "Epoch 57/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 1.0113 - mean_absolute_error: 0.8046 - val_loss: 1.6183 - val_mean_absolute_error: 1.0724\n",
      "Epoch 58/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.9784 - mean_absolute_error: 0.7769 - val_loss: 1.8124 - val_mean_absolute_error: 1.1146\n",
      "Epoch 59/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.9697 - mean_absolute_error: 0.7868 - val_loss: 1.6786 - val_mean_absolute_error: 1.0855\n",
      "Epoch 60/145\n",
      "800/800 [==============================] - 0s 160us/step - loss: 0.9381 - mean_absolute_error: 0.7562 - val_loss: 1.8107 - val_mean_absolute_error: 1.1017\n",
      "Epoch 61/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 1.0078 - mean_absolute_error: 0.7940 - val_loss: 1.5721 - val_mean_absolute_error: 1.0287\n",
      "Epoch 62/145\n",
      "800/800 [==============================] - 0s 157us/step - loss: 0.9312 - mean_absolute_error: 0.7553 - val_loss: 1.4994 - val_mean_absolute_error: 1.0019\n",
      "Epoch 63/145\n",
      "800/800 [==============================] - 0s 159us/step - loss: 0.9379 - mean_absolute_error: 0.7705 - val_loss: 1.6084 - val_mean_absolute_error: 1.0179\n",
      "Epoch 64/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.9202 - mean_absolute_error: 0.7655 - val_loss: 1.8122 - val_mean_absolute_error: 1.0909\n",
      "Epoch 65/145\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.9375 - mean_absolute_error: 0.7723 - val_loss: 1.8529 - val_mean_absolute_error: 1.1062\n",
      "Epoch 66/145\n",
      "800/800 [==============================] - 0s 155us/step - loss: 0.9119 - mean_absolute_error: 0.7493 - val_loss: 1.5561 - val_mean_absolute_error: 1.0101\n",
      "Epoch 67/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.8803 - mean_absolute_error: 0.7360 - val_loss: 1.7075 - val_mean_absolute_error: 1.0618\n",
      "Epoch 68/145\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.9220 - mean_absolute_error: 0.7639 - val_loss: 1.5863 - val_mean_absolute_error: 1.0556\n",
      "Epoch 69/145\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.8793 - mean_absolute_error: 0.7365 - val_loss: 1.8067 - val_mean_absolute_error: 1.0957\n",
      "Epoch 70/145\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.9270 - mean_absolute_error: 0.7685 - val_loss: 1.6993 - val_mean_absolute_error: 1.0753\n",
      "Epoch 71/145\n",
      "800/800 [==============================] - 0s 160us/step - loss: 1.1088 - mean_absolute_error: 0.8275 - val_loss: 1.5421 - val_mean_absolute_error: 1.0086\n",
      "Epoch 72/145\n",
      "800/800 [==============================] - 0s 157us/step - loss: 1.1172 - mean_absolute_error: 0.8481 - val_loss: 1.5920 - val_mean_absolute_error: 1.0275\n",
      "Epoch 73/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.9783 - mean_absolute_error: 0.7892 - val_loss: 1.6909 - val_mean_absolute_error: 1.0650\n",
      "Epoch 74/145\n",
      "800/800 [==============================] - 0s 159us/step - loss: 0.9639 - mean_absolute_error: 0.7850 - val_loss: 1.7016 - val_mean_absolute_error: 1.0456\n",
      "Epoch 75/145\n",
      "800/800 [==============================] - 0s 169us/step - loss: 0.9706 - mean_absolute_error: 0.7986 - val_loss: 1.6011 - val_mean_absolute_error: 1.0502\n",
      "Epoch 76/145\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.9548 - mean_absolute_error: 0.7790 - val_loss: 1.8925 - val_mean_absolute_error: 1.1176\n",
      "Epoch 77/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.9217 - mean_absolute_error: 0.7635 - val_loss: 2.0045 - val_mean_absolute_error: 1.1480\n",
      "Epoch 78/145\n",
      "800/800 [==============================] - 0s 174us/step - loss: 0.9353 - mean_absolute_error: 0.7668 - val_loss: 1.9493 - val_mean_absolute_error: 1.1420\n",
      "Epoch 79/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.9189 - mean_absolute_error: 0.7657 - val_loss: 1.6989 - val_mean_absolute_error: 1.1002\n",
      "Epoch 80/145\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.9622 - mean_absolute_error: 0.7850 - val_loss: 1.5150 - val_mean_absolute_error: 1.0058\n",
      "Epoch 81/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.9476 - mean_absolute_error: 0.7788 - val_loss: 1.6862 - val_mean_absolute_error: 1.0718\n",
      "Epoch 82/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.9581 - mean_absolute_error: 0.7828 - val_loss: 1.8350 - val_mean_absolute_error: 1.1217\n",
      "Epoch 83/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 0.9645 - mean_absolute_error: 0.7777 - val_loss: 1.7742 - val_mean_absolute_error: 1.0950\n",
      "Epoch 84/145\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.9433 - mean_absolute_error: 0.7631 - val_loss: 1.4941 - val_mean_absolute_error: 1.0080\n",
      "Epoch 85/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 0.9390 - mean_absolute_error: 0.7684 - val_loss: 1.5146 - val_mean_absolute_error: 1.0283\n",
      "Epoch 86/145\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.9074 - mean_absolute_error: 0.7579 - val_loss: 1.4870 - val_mean_absolute_error: 1.0055\n",
      "Epoch 87/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.9175 - mean_absolute_error: 0.7623 - val_loss: 1.4606 - val_mean_absolute_error: 1.0094\n",
      "Epoch 88/145\n",
      "800/800 [==============================] - 0s 187us/step - loss: 0.9000 - mean_absolute_error: 0.7614 - val_loss: 1.5168 - val_mean_absolute_error: 1.0428\n",
      "Epoch 89/145\n",
      "800/800 [==============================] - 0s 174us/step - loss: 0.9152 - mean_absolute_error: 0.7643 - val_loss: 1.6050 - val_mean_absolute_error: 1.0657\n",
      "Epoch 90/145\n",
      "800/800 [==============================] - 0s 174us/step - loss: 0.8942 - mean_absolute_error: 0.7454 - val_loss: 1.6191 - val_mean_absolute_error: 1.0591\n",
      "Epoch 91/145\n",
      "800/800 [==============================] - 0s 174us/step - loss: 0.8707 - mean_absolute_error: 0.7434 - val_loss: 1.6013 - val_mean_absolute_error: 1.0459\n",
      "Epoch 92/145\n",
      "800/800 [==============================] - 0s 174us/step - loss: 0.8890 - mean_absolute_error: 0.7491 - val_loss: 1.6690 - val_mean_absolute_error: 1.0650\n",
      "Epoch 93/145\n",
      "800/800 [==============================] - 0s 174us/step - loss: 0.9430 - mean_absolute_error: 0.7702 - val_loss: 1.9206 - val_mean_absolute_error: 1.1307\n",
      "Epoch 94/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.8862 - mean_absolute_error: 0.7604 - val_loss: 1.6512 - val_mean_absolute_error: 1.0640\n",
      "Epoch 95/145\n",
      "800/800 [==============================] - 0s 174us/step - loss: 0.8408 - mean_absolute_error: 0.7266 - val_loss: 1.4571 - val_mean_absolute_error: 1.0118\n",
      "Epoch 96/145\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.8273 - mean_absolute_error: 0.7287 - val_loss: 1.5523 - val_mean_absolute_error: 1.0509\n",
      "Epoch 97/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 0.8319 - mean_absolute_error: 0.7186 - val_loss: 1.6029 - val_mean_absolute_error: 1.0751\n",
      "Epoch 98/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.8533 - mean_absolute_error: 0.7345 - val_loss: 1.4743 - val_mean_absolute_error: 1.0252\n",
      "Epoch 99/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.8645 - mean_absolute_error: 0.7431 - val_loss: 1.6942 - val_mean_absolute_error: 1.0860\n",
      "Epoch 100/145\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.8853 - mean_absolute_error: 0.7489 - val_loss: 1.6209 - val_mean_absolute_error: 1.0576\n",
      "Epoch 101/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 0.8733 - mean_absolute_error: 0.7424 - val_loss: 1.6529 - val_mean_absolute_error: 1.0627\n",
      "Epoch 102/145\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.8687 - mean_absolute_error: 0.7511 - val_loss: 1.6135 - val_mean_absolute_error: 1.0786\n",
      "Epoch 103/145\n",
      "800/800 [==============================] - 0s 171us/step - loss: 0.8982 - mean_absolute_error: 0.7584 - val_loss: 1.6015 - val_mean_absolute_error: 1.0549\n",
      "Epoch 104/145\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.8888 - mean_absolute_error: 0.7525 - val_loss: 1.8348 - val_mean_absolute_error: 1.1583\n",
      "Epoch 105/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 0.9060 - mean_absolute_error: 0.7519 - val_loss: 1.8224 - val_mean_absolute_error: 1.1265\n",
      "Epoch 106/145\n",
      "800/800 [==============================] - 0s 181us/step - loss: 0.8756 - mean_absolute_error: 0.7494 - val_loss: 1.6043 - val_mean_absolute_error: 1.0354\n",
      "Epoch 107/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.8661 - mean_absolute_error: 0.7472 - val_loss: 1.5667 - val_mean_absolute_error: 1.0282\n",
      "Epoch 108/145\n",
      "800/800 [==============================] - 0s 171us/step - loss: 0.9271 - mean_absolute_error: 0.7685 - val_loss: 1.5598 - val_mean_absolute_error: 1.0250\n",
      "Epoch 109/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.8506 - mean_absolute_error: 0.7344 - val_loss: 1.4295 - val_mean_absolute_error: 0.9832\n",
      "Epoch 110/145\n",
      "800/800 [==============================] - 0s 180us/step - loss: 0.8953 - mean_absolute_error: 0.7464 - val_loss: 1.4523 - val_mean_absolute_error: 1.0037\n",
      "Epoch 111/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.9117 - mean_absolute_error: 0.7488 - val_loss: 1.4829 - val_mean_absolute_error: 1.0174\n",
      "Epoch 112/145\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.8931 - mean_absolute_error: 0.7534 - val_loss: 1.4606 - val_mean_absolute_error: 1.0174\n",
      "Epoch 113/145\n",
      "800/800 [==============================] - 0s 171us/step - loss: 0.9004 - mean_absolute_error: 0.7572 - val_loss: 1.4004 - val_mean_absolute_error: 0.9628\n",
      "Epoch 114/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 0.8942 - mean_absolute_error: 0.7488 - val_loss: 1.7528 - val_mean_absolute_error: 1.0855\n",
      "Epoch 115/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.9466 - mean_absolute_error: 0.7793 - val_loss: 1.7558 - val_mean_absolute_error: 1.0863\n",
      "Epoch 116/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.8884 - mean_absolute_error: 0.7472 - val_loss: 1.6334 - val_mean_absolute_error: 1.0377\n",
      "Epoch 117/145\n",
      "800/800 [==============================] - 0s 171us/step - loss: 0.9312 - mean_absolute_error: 0.7725 - val_loss: 1.5302 - val_mean_absolute_error: 1.0294\n",
      "Epoch 118/145\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.9071 - mean_absolute_error: 0.7621 - val_loss: 1.5844 - val_mean_absolute_error: 1.0383\n",
      "Epoch 119/145\n",
      "800/800 [==============================] - 0s 171us/step - loss: 0.9948 - mean_absolute_error: 0.7794 - val_loss: 1.6627 - val_mean_absolute_error: 1.0997\n",
      "Epoch 120/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 0.9860 - mean_absolute_error: 0.7967 - val_loss: 1.5319 - val_mean_absolute_error: 1.0759\n",
      "Epoch 121/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 0.9694 - mean_absolute_error: 0.7824 - val_loss: 1.4615 - val_mean_absolute_error: 1.0095\n",
      "Epoch 122/145\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.9581 - mean_absolute_error: 0.7843 - val_loss: 1.6599 - val_mean_absolute_error: 1.0722\n",
      "Epoch 123/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 0.9218 - mean_absolute_error: 0.7614 - val_loss: 1.4329 - val_mean_absolute_error: 1.0108\n",
      "Epoch 124/145\n",
      "800/800 [==============================] - 0s 178us/step - loss: 0.9156 - mean_absolute_error: 0.7633 - val_loss: 1.4613 - val_mean_absolute_error: 1.0417\n",
      "Epoch 125/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 0.9100 - mean_absolute_error: 0.7510 - val_loss: 1.5455 - val_mean_absolute_error: 1.0566\n",
      "Epoch 126/145\n",
      "800/800 [==============================] - 0s 177us/step - loss: 0.9655 - mean_absolute_error: 0.7708 - val_loss: 1.5495 - val_mean_absolute_error: 1.0641\n",
      "Epoch 127/145\n",
      "800/800 [==============================] - 0s 179us/step - loss: 0.9461 - mean_absolute_error: 0.7719 - val_loss: 1.4776 - val_mean_absolute_error: 1.0118\n",
      "Epoch 128/145\n",
      "800/800 [==============================] - 0s 174us/step - loss: 0.9512 - mean_absolute_error: 0.7702 - val_loss: 1.5152 - val_mean_absolute_error: 1.0214\n",
      "Epoch 129/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 0.9151 - mean_absolute_error: 0.7645 - val_loss: 1.9491 - val_mean_absolute_error: 1.1515\n",
      "Epoch 130/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 0.9544 - mean_absolute_error: 0.7783 - val_loss: 1.4842 - val_mean_absolute_error: 0.9925\n",
      "Epoch 131/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 0.9755 - mean_absolute_error: 0.7729 - val_loss: 1.5590 - val_mean_absolute_error: 1.0398\n",
      "Epoch 132/145\n",
      "800/800 [==============================] - 0s 182us/step - loss: 0.9180 - mean_absolute_error: 0.7614 - val_loss: 1.8611 - val_mean_absolute_error: 1.1155\n",
      "Epoch 133/145\n",
      "800/800 [==============================] - 0s 183us/step - loss: 0.9283 - mean_absolute_error: 0.7655 - val_loss: 1.5765 - val_mean_absolute_error: 1.0276\n",
      "Epoch 134/145\n",
      "800/800 [==============================] - 0s 178us/step - loss: 0.9173 - mean_absolute_error: 0.7599 - val_loss: 1.8221 - val_mean_absolute_error: 1.1238\n",
      "Epoch 135/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 0.9162 - mean_absolute_error: 0.7696 - val_loss: 1.5338 - val_mean_absolute_error: 1.0225\n",
      "Epoch 136/145\n",
      "800/800 [==============================] - 0s 177us/step - loss: 0.9065 - mean_absolute_error: 0.7551 - val_loss: 1.4719 - val_mean_absolute_error: 1.0006\n",
      "Epoch 137/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 0.8982 - mean_absolute_error: 0.7587 - val_loss: 1.6003 - val_mean_absolute_error: 1.0438\n",
      "Epoch 138/145\n",
      "800/800 [==============================] - 0s 178us/step - loss: 0.9085 - mean_absolute_error: 0.7547 - val_loss: 1.4411 - val_mean_absolute_error: 0.9783\n",
      "Epoch 139/145\n",
      "800/800 [==============================] - 0s 177us/step - loss: 0.9642 - mean_absolute_error: 0.7776 - val_loss: 1.4812 - val_mean_absolute_error: 0.9790\n",
      "Epoch 140/145\n",
      "800/800 [==============================] - 0s 181us/step - loss: 0.8905 - mean_absolute_error: 0.7574 - val_loss: 1.5996 - val_mean_absolute_error: 1.0453\n",
      "Epoch 141/145\n",
      "800/800 [==============================] - 0s 177us/step - loss: 0.9048 - mean_absolute_error: 0.7581 - val_loss: 1.4387 - val_mean_absolute_error: 0.9809\n",
      "Epoch 142/145\n",
      "800/800 [==============================] - 0s 179us/step - loss: 0.9344 - mean_absolute_error: 0.7616 - val_loss: 1.4676 - val_mean_absolute_error: 0.9881\n",
      "Epoch 143/145\n",
      "800/800 [==============================] - 0s 191us/step - loss: 0.8895 - mean_absolute_error: 0.7579 - val_loss: 1.5928 - val_mean_absolute_error: 1.0692\n",
      "Epoch 144/145\n",
      "800/800 [==============================] - 0s 175us/step - loss: 0.8993 - mean_absolute_error: 0.7628 - val_loss: 1.5082 - val_mean_absolute_error: 1.0071\n",
      "Epoch 145/145\n",
      "800/800 [==============================] - 0s 176us/step - loss: 0.8840 - mean_absolute_error: 0.7438 - val_loss: 1.5081 - val_mean_absolute_error: 1.0068\n"
     ]
    }
   ],
   "source": [
    "model,loss = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7cd937d3b270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m145\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-ae5b4c18ef06>\u001b[0m in \u001b[0;36mplot_losses\u001b[0;34m(epochs, history)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m#ks = history.keys()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mks\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val_mean_absolute_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'x-'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ae5b4c18ef06>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m#ks = history.keys()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mks\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val_mean_absolute_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'x-'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plot_losses(145,loss.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon",
   "language": "python",
   "name": "datathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
