{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0,'/p/project/training2005/jupyter/kernels/tensorflow_test/lib/python3.6/site-packages')\n",
    "# import tensorflow\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "# import tensorflow.keras.layers.Attention as Attention\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# ttd = np.load('redttd.npy')\n",
    "# ptd = np.load('redptd.npy')\n",
    "# ntd = np.load('/p/project/training2005/HZG_Challenge/nao_index_train.npy')\n",
    "\n",
    "# ttst = np.load('redttst.npy')\n",
    "# ptst = np.load('redptst.npy')\n",
    "\n",
    "ttd = np.load('/p/project/training2005/HZG_Challenge/tas_train.npy')\n",
    "ptd = np.load('/p/project/training2005/HZG_Challenge/psl_train.npy')\n",
    "ntd = np.load('/p/project/training2005/HZG_Challenge/nao_index_train.npy')\n",
    "\n",
    "ttst = np.load('/p/project/training2005/HZG_Challenge/tas_predict.npy')\n",
    "ptst = np.load('/p/project/training2005/HZG_Challenge/psl_predict.npy')\n",
    "\n",
    "#plt.plot(range(data.shape[1]),data[0])\n",
    "\n",
    "def branch_graph(dim, input_tensor):\n",
    "    x = layers.Dense(dim, activation='relu')(input_tensor)\n",
    "    x = layers.Dense(dim, activation='relu')(x)\n",
    "    x = layers.Dense(dim, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(dim, activation='relu')(x)\n",
    "    x = layers.Dense(dim, activation='relu')(x)\n",
    "    x = layers.Dense(dim, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(dim, activation='relu')(x)\n",
    "    x = layers.Dense(dim, activation='relu')(x)\n",
    "    x = layers.Dense(dim, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x1 = x\n",
    "    x = layers.Flatten()(x)\n",
    "    x1 = layers.Dense(dim, activation='sigmoid')(x1)\n",
    "    x1 = layers.Flatten()(x1)\n",
    "    x = layers.Multiply()([x,x1])\n",
    "#     x = Attention()([x,x1])\n",
    "    return x\n",
    "    \n",
    "def build_model():\n",
    "    inp1 = Input(shape=(2322,1))\n",
    "    inp2 = Input(shape=(2322,1))\n",
    "    \n",
    "    #first branch\n",
    "    x1 = branch_graph(3,inp1)\n",
    "    \n",
    "    #second branch\n",
    "    x2 = branch_graph(3,inp2)\n",
    "\n",
    "    #merge branches \n",
    "    out = layers.Concatenate()([x1,x2])\n",
    "#     out = layers.Flatten()(out)\n",
    "    out = layers.Dense(1)(out)\n",
    "    \n",
    "    model = Model([inp1,inp2],out)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def train_model(model, params ={'vsplit':0.111,'ne':145,'bs':256}):\n",
    "#     d1 = \n",
    "    history = model.fit([np.reshape(ttd,(ttd.shape[0],ttd.shape[1],1)),\n",
    "                         np.reshape(ptd,(ptd.shape[0],ptd.shape[1],1))], \n",
    "                        np.reshape(ntd,(ntd.shape[0],1)),\n",
    "                        epochs=params['ne'], \n",
    "                        batch_size=params['bs'],\n",
    "                        validation_split=params['vsplit'])\n",
    "    return [model, history]\n",
    "\n",
    "def test_model(model):\n",
    "    return model.predict([ttst,ptst])\n",
    "\n",
    "def plot_losses(epochs, history):\n",
    "    #ks = history.keys()\n",
    "    ks =['loss','val_loss','mean_absolute_error','val_mean_absolute_error']\n",
    "    [plt.plot(range(epochs), history[k],'x-') for k in ks]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2322, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2322, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2322, 3)      6           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 2322, 3)      6           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2322, 3)      12          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 2322, 3)      12          dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2322, 3)      12          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 2322, 3)      12          dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 2322, 3)      12          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 2322, 3)      12          dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2322, 3)      12          batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2322, 3)      12          batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2322, 3)      12          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 2322, 3)      12          dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2322, 3)      12          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 2322, 3)      12          dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 2322, 3)      12          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 2322, 3)      12          dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2322, 3)      12          batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 2322, 3)      12          batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2322, 3)      12          dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 2322, 3)      12          dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2322, 3)      12          dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 2322, 3)      12          dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 2322, 3)      12          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 2322, 3)      12          dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 2322, 3)      12          batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 2322, 3)      12          batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 6966)         0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 6966)         0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 6966)         0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 6966)         0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 6966)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 6966)         0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 13932)        0           multiply[0][0]                   \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1)            13933       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 14,233\n",
      "Trainable params: 14,197\n",
      "Non-trainable params: 36\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvocationException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jusuf/stages/Devel-2019a/software/Python/3.6.8-GCCcore-8.3.0/lib/python3.6/site-packages/pydotplus-2.0.2-py3.6.egg/pydotplus/graphviz.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format)\u001b[0m\n\u001b[1;32m   1959\u001b[0m                 raise InvocationException(\n\u001b[0;32m-> 1960\u001b[0;31m                     'GraphViz\\'s executables not found')\n\u001b[0m\u001b[1;32m   1961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvocationException\u001b[0m: GraphViz's executables not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-999b2536de98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    146\u001b[0m           \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m   \"\"\"\n\u001b[0;32m--> 148\u001b[0;31m   \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m   \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# pydot raises a generic Exception here,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# so no specific class can be caught.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[1;32m     50\u001b[0m                       ' and graphviz for `pydotprint` to work.')\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 100 samples\n",
      "Epoch 1/145\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 1.0211 - mean_absolute_error: 0.8094 - val_loss: 1.2592 - val_mean_absolute_error: 0.9403\n",
      "Epoch 2/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.9107 - mean_absolute_error: 0.7587 - val_loss: 1.2627 - val_mean_absolute_error: 0.9645\n",
      "Epoch 3/145\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.8870 - mean_absolute_error: 0.7473 - val_loss: 1.3215 - val_mean_absolute_error: 0.9463\n",
      "Epoch 4/145\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.8901 - mean_absolute_error: 0.7423 - val_loss: 1.3011 - val_mean_absolute_error: 0.9620\n",
      "Epoch 5/145\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.8604 - mean_absolute_error: 0.7435 - val_loss: 1.3274 - val_mean_absolute_error: 0.9683\n",
      "Epoch 6/145\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.8138 - mean_absolute_error: 0.7178 - val_loss: 1.3668 - val_mean_absolute_error: 0.9512\n",
      "Epoch 7/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.8170 - mean_absolute_error: 0.7148 - val_loss: 1.2900 - val_mean_absolute_error: 0.9642\n",
      "Epoch 8/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.7629 - mean_absolute_error: 0.6982 - val_loss: 1.2816 - val_mean_absolute_error: 0.9588\n",
      "Epoch 9/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.7443 - mean_absolute_error: 0.6873 - val_loss: 1.2843 - val_mean_absolute_error: 0.9438\n",
      "Epoch 10/145\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.7409 - mean_absolute_error: 0.6800 - val_loss: 1.2673 - val_mean_absolute_error: 0.9401\n",
      "Epoch 11/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.7150 - mean_absolute_error: 0.6664 - val_loss: 1.2610 - val_mean_absolute_error: 0.9560\n",
      "Epoch 12/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.6958 - mean_absolute_error: 0.6567 - val_loss: 1.2890 - val_mean_absolute_error: 0.9761\n",
      "Epoch 13/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.6862 - mean_absolute_error: 0.6527 - val_loss: 1.3035 - val_mean_absolute_error: 0.9771\n",
      "Epoch 14/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.6636 - mean_absolute_error: 0.6428 - val_loss: 1.3048 - val_mean_absolute_error: 0.9667\n",
      "Epoch 15/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.6396 - mean_absolute_error: 0.6415 - val_loss: 1.3123 - val_mean_absolute_error: 0.9680\n",
      "Epoch 16/145\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.6219 - mean_absolute_error: 0.6339 - val_loss: 1.3037 - val_mean_absolute_error: 0.9446\n",
      "Epoch 17/145\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.6060 - mean_absolute_error: 0.6139 - val_loss: 1.3302 - val_mean_absolute_error: 0.9619\n",
      "Epoch 18/145\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.5896 - mean_absolute_error: 0.6060 - val_loss: 1.3340 - val_mean_absolute_error: 0.9820\n",
      "Epoch 19/145\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.5704 - mean_absolute_error: 0.6011 - val_loss: 1.3505 - val_mean_absolute_error: 0.9846\n",
      "Epoch 20/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.5516 - mean_absolute_error: 0.5882 - val_loss: 1.3671 - val_mean_absolute_error: 0.9756\n",
      "Epoch 21/145\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.5263 - mean_absolute_error: 0.5737 - val_loss: 1.3730 - val_mean_absolute_error: 0.9913\n",
      "Epoch 22/145\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.5122 - mean_absolute_error: 0.5692 - val_loss: 1.3574 - val_mean_absolute_error: 0.9830\n",
      "Epoch 23/145\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.4893 - mean_absolute_error: 0.5510 - val_loss: 1.3688 - val_mean_absolute_error: 0.9866\n",
      "Epoch 24/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.4721 - mean_absolute_error: 0.5424 - val_loss: 1.3729 - val_mean_absolute_error: 0.9953\n",
      "Epoch 25/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.4562 - mean_absolute_error: 0.5293 - val_loss: 1.3744 - val_mean_absolute_error: 0.9906\n",
      "Epoch 26/145\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.4355 - mean_absolute_error: 0.5188 - val_loss: 1.3807 - val_mean_absolute_error: 0.9989\n",
      "Epoch 27/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.4248 - mean_absolute_error: 0.5110 - val_loss: 1.3862 - val_mean_absolute_error: 0.9973\n",
      "Epoch 28/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.4153 - mean_absolute_error: 0.5043 - val_loss: 1.3872 - val_mean_absolute_error: 1.0074\n",
      "Epoch 29/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.3843 - mean_absolute_error: 0.4842 - val_loss: 1.4074 - val_mean_absolute_error: 0.9904\n",
      "Epoch 30/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.3735 - mean_absolute_error: 0.4794 - val_loss: 1.4298 - val_mean_absolute_error: 1.0202\n",
      "Epoch 31/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.3858 - mean_absolute_error: 0.4854 - val_loss: 1.4086 - val_mean_absolute_error: 1.0001\n",
      "Epoch 32/145\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.3504 - mean_absolute_error: 0.4625 - val_loss: 1.4424 - val_mean_absolute_error: 1.0243\n",
      "Epoch 33/145\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.3424 - mean_absolute_error: 0.4550 - val_loss: 1.4874 - val_mean_absolute_error: 1.0408\n",
      "Epoch 34/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.3184 - mean_absolute_error: 0.4438 - val_loss: 1.4869 - val_mean_absolute_error: 1.0281\n",
      "Epoch 35/145\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.3055 - mean_absolute_error: 0.4296 - val_loss: 1.4769 - val_mean_absolute_error: 1.0513\n",
      "Epoch 36/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.2910 - mean_absolute_error: 0.4227 - val_loss: 1.4882 - val_mean_absolute_error: 1.0293\n",
      "Epoch 37/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.2745 - mean_absolute_error: 0.4053 - val_loss: 1.5062 - val_mean_absolute_error: 1.0408\n",
      "Epoch 38/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.2601 - mean_absolute_error: 0.3948 - val_loss: 1.5200 - val_mean_absolute_error: 1.0411\n",
      "Epoch 39/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.2452 - mean_absolute_error: 0.3821 - val_loss: 1.5286 - val_mean_absolute_error: 1.0535\n",
      "Epoch 40/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.2335 - mean_absolute_error: 0.3701 - val_loss: 1.5432 - val_mean_absolute_error: 1.0570\n",
      "Epoch 41/145\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.2270 - mean_absolute_error: 0.3645 - val_loss: 1.5626 - val_mean_absolute_error: 1.0613\n",
      "Epoch 42/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.2002 - mean_absolute_error: 0.3458 - val_loss: 1.5865 - val_mean_absolute_error: 1.0638\n",
      "Epoch 43/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1948 - mean_absolute_error: 0.3391 - val_loss: 1.5939 - val_mean_absolute_error: 1.0714\n",
      "Epoch 44/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1836 - mean_absolute_error: 0.3294 - val_loss: 1.6016 - val_mean_absolute_error: 1.0545\n",
      "Epoch 45/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1868 - mean_absolute_error: 0.3376 - val_loss: 1.6043 - val_mean_absolute_error: 1.0612\n",
      "Epoch 46/145\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1635 - mean_absolute_error: 0.3106 - val_loss: 1.6252 - val_mean_absolute_error: 1.0859\n",
      "Epoch 47/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1502 - mean_absolute_error: 0.2964 - val_loss: 1.6409 - val_mean_absolute_error: 1.0962\n",
      "Epoch 48/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1467 - mean_absolute_error: 0.2952 - val_loss: 1.6657 - val_mean_absolute_error: 1.0814\n",
      "Epoch 49/145\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1362 - mean_absolute_error: 0.2865 - val_loss: 1.6917 - val_mean_absolute_error: 1.0944\n",
      "Epoch 50/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1276 - mean_absolute_error: 0.2738 - val_loss: 1.7081 - val_mean_absolute_error: 1.1070\n",
      "Epoch 51/145\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1248 - mean_absolute_error: 0.2757 - val_loss: 1.7172 - val_mean_absolute_error: 1.1205\n",
      "Epoch 52/145\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1175 - mean_absolute_error: 0.2679 - val_loss: 1.7209 - val_mean_absolute_error: 1.1150\n",
      "Epoch 53/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1065 - mean_absolute_error: 0.2534 - val_loss: 1.7193 - val_mean_absolute_error: 1.1068\n",
      "Epoch 54/145\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.0964 - mean_absolute_error: 0.2402 - val_loss: 1.6896 - val_mean_absolute_error: 1.0992\n",
      "Epoch 55/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0941 - mean_absolute_error: 0.2400 - val_loss: 1.7006 - val_mean_absolute_error: 1.1062\n",
      "Epoch 56/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0836 - mean_absolute_error: 0.2246 - val_loss: 1.7193 - val_mean_absolute_error: 1.1134\n",
      "Epoch 57/145\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.0782 - mean_absolute_error: 0.2153 - val_loss: 1.7259 - val_mean_absolute_error: 1.1099\n",
      "Epoch 58/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0780 - mean_absolute_error: 0.2181 - val_loss: 1.7408 - val_mean_absolute_error: 1.1314\n",
      "Epoch 59/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0879 - mean_absolute_error: 0.2327 - val_loss: 1.7526 - val_mean_absolute_error: 1.1126\n",
      "Epoch 60/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0896 - mean_absolute_error: 0.2320 - val_loss: 1.7374 - val_mean_absolute_error: 1.1263\n",
      "Epoch 61/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0778 - mean_absolute_error: 0.2168 - val_loss: 1.7367 - val_mean_absolute_error: 1.1094\n",
      "Epoch 62/145\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.0709 - mean_absolute_error: 0.2103 - val_loss: 1.7442 - val_mean_absolute_error: 1.1229\n",
      "Epoch 63/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0744 - mean_absolute_error: 0.2119 - val_loss: 1.7846 - val_mean_absolute_error: 1.1153\n",
      "Epoch 64/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0611 - mean_absolute_error: 0.1946 - val_loss: 1.8046 - val_mean_absolute_error: 1.1442\n",
      "Epoch 65/145\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.0678 - mean_absolute_error: 0.2044 - val_loss: 1.8183 - val_mean_absolute_error: 1.1361\n",
      "Epoch 66/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0519 - mean_absolute_error: 0.1806 - val_loss: 1.8270 - val_mean_absolute_error: 1.1452\n",
      "Epoch 67/145\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.0439 - mean_absolute_error: 0.1608 - val_loss: 1.8262 - val_mean_absolute_error: 1.1407\n",
      "Epoch 68/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0408 - mean_absolute_error: 0.1560 - val_loss: 1.8074 - val_mean_absolute_error: 1.1361\n",
      "Epoch 69/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0372 - mean_absolute_error: 0.1514 - val_loss: 1.8142 - val_mean_absolute_error: 1.1431\n",
      "Epoch 70/145\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.0411 - mean_absolute_error: 0.1592 - val_loss: 1.8132 - val_mean_absolute_error: 1.1339\n",
      "Epoch 71/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0356 - mean_absolute_error: 0.1473 - val_loss: 1.8414 - val_mean_absolute_error: 1.1502\n",
      "Epoch 72/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0307 - mean_absolute_error: 0.1352 - val_loss: 1.8645 - val_mean_absolute_error: 1.1463\n",
      "Epoch 73/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0315 - mean_absolute_error: 0.1398 - val_loss: 1.8888 - val_mean_absolute_error: 1.1624\n",
      "Epoch 74/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0268 - mean_absolute_error: 0.1257 - val_loss: 1.8794 - val_mean_absolute_error: 1.1523\n",
      "Epoch 75/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0310 - mean_absolute_error: 0.1374 - val_loss: 1.9081 - val_mean_absolute_error: 1.1613\n",
      "Epoch 76/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0240 - mean_absolute_error: 0.1224 - val_loss: 1.8776 - val_mean_absolute_error: 1.1546\n",
      "Epoch 77/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0239 - mean_absolute_error: 0.1214 - val_loss: 1.8866 - val_mean_absolute_error: 1.1659\n",
      "Epoch 78/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0223 - mean_absolute_error: 0.1161 - val_loss: 1.8920 - val_mean_absolute_error: 1.1542\n",
      "Epoch 79/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0241 - mean_absolute_error: 0.1212 - val_loss: 1.8931 - val_mean_absolute_error: 1.1714\n",
      "Epoch 80/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0270 - mean_absolute_error: 0.1304 - val_loss: 1.9019 - val_mean_absolute_error: 1.1657\n",
      "Epoch 81/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0196 - mean_absolute_error: 0.1102 - val_loss: 1.9126 - val_mean_absolute_error: 1.1689\n",
      "Epoch 82/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0189 - mean_absolute_error: 0.1086 - val_loss: 1.9136 - val_mean_absolute_error: 1.1605\n",
      "Epoch 83/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0164 - mean_absolute_error: 0.1003 - val_loss: 1.9260 - val_mean_absolute_error: 1.1754\n",
      "Epoch 84/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0157 - mean_absolute_error: 0.0988 - val_loss: 1.9404 - val_mean_absolute_error: 1.1784\n",
      "Epoch 85/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0146 - mean_absolute_error: 0.0932 - val_loss: 1.9508 - val_mean_absolute_error: 1.1879\n",
      "Epoch 86/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0152 - mean_absolute_error: 0.0938 - val_loss: 1.9443 - val_mean_absolute_error: 1.1743\n",
      "Epoch 87/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0126 - mean_absolute_error: 0.0900 - val_loss: 1.9286 - val_mean_absolute_error: 1.1730\n",
      "Epoch 88/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0208 - mean_absolute_error: 0.1133 - val_loss: 1.9351 - val_mean_absolute_error: 1.1669\n",
      "Epoch 89/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0140 - mean_absolute_error: 0.0931 - val_loss: 1.9650 - val_mean_absolute_error: 1.1845\n",
      "Epoch 90/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0179 - mean_absolute_error: 0.1053 - val_loss: 1.9647 - val_mean_absolute_error: 1.1817\n",
      "Epoch 91/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0163 - mean_absolute_error: 0.0987 - val_loss: 1.9152 - val_mean_absolute_error: 1.1693\n",
      "Epoch 92/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0144 - mean_absolute_error: 0.0953 - val_loss: 1.9087 - val_mean_absolute_error: 1.1711\n",
      "Epoch 93/145\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.0162 - mean_absolute_error: 0.0994 - val_loss: 1.9368 - val_mean_absolute_error: 1.1802\n",
      "Epoch 94/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0103 - mean_absolute_error: 0.0778 - val_loss: 1.9702 - val_mean_absolute_error: 1.1910\n",
      "Epoch 95/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0122 - mean_absolute_error: 0.0865 - val_loss: 1.9742 - val_mean_absolute_error: 1.1847\n",
      "Epoch 96/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0122 - mean_absolute_error: 0.0854 - val_loss: 1.9619 - val_mean_absolute_error: 1.1818\n",
      "Epoch 97/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0107 - mean_absolute_error: 0.0800 - val_loss: 1.9556 - val_mean_absolute_error: 1.1678\n",
      "Epoch 98/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0118 - mean_absolute_error: 0.0847 - val_loss: 1.9607 - val_mean_absolute_error: 1.1839\n",
      "Epoch 99/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0118 - mean_absolute_error: 0.0842 - val_loss: 1.9567 - val_mean_absolute_error: 1.1868\n",
      "Epoch 100/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0102 - mean_absolute_error: 0.0792 - val_loss: 1.9612 - val_mean_absolute_error: 1.1856\n",
      "Epoch 101/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0102 - mean_absolute_error: 0.0789 - val_loss: 1.9551 - val_mean_absolute_error: 1.1798\n",
      "Epoch 102/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0102 - mean_absolute_error: 0.0796 - val_loss: 1.9402 - val_mean_absolute_error: 1.1753\n",
      "Epoch 103/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0098 - mean_absolute_error: 0.0788 - val_loss: 1.9574 - val_mean_absolute_error: 1.1855\n",
      "Epoch 104/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0099 - mean_absolute_error: 0.0788 - val_loss: 1.9751 - val_mean_absolute_error: 1.1849\n",
      "Epoch 105/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0111 - mean_absolute_error: 0.0831 - val_loss: 1.9790 - val_mean_absolute_error: 1.1885\n",
      "Epoch 106/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0070 - mean_absolute_error: 0.0659 - val_loss: 1.9802 - val_mean_absolute_error: 1.1829\n",
      "Epoch 107/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0093 - mean_absolute_error: 0.0737 - val_loss: 1.9739 - val_mean_absolute_error: 1.1843\n",
      "Epoch 108/145\n",
      "800/800 [==============================] - 0s 154us/step - loss: 0.0070 - mean_absolute_error: 0.0665 - val_loss: 1.9606 - val_mean_absolute_error: 1.1767\n",
      "Epoch 109/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0059 - mean_absolute_error: 0.0603 - val_loss: 1.9568 - val_mean_absolute_error: 1.1819\n",
      "Epoch 110/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0112 - mean_absolute_error: 0.0768 - val_loss: 1.9506 - val_mean_absolute_error: 1.1731\n",
      "Epoch 111/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0132 - mean_absolute_error: 0.0874 - val_loss: 1.9780 - val_mean_absolute_error: 1.1773\n",
      "Epoch 112/145\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.0207 - mean_absolute_error: 0.1163 - val_loss: 1.9980 - val_mean_absolute_error: 1.1971\n",
      "Epoch 113/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0163 - mean_absolute_error: 0.0986 - val_loss: 1.9535 - val_mean_absolute_error: 1.1715\n",
      "Epoch 114/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0174 - mean_absolute_error: 0.1042 - val_loss: 1.9464 - val_mean_absolute_error: 1.1881\n",
      "Epoch 115/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0120 - mean_absolute_error: 0.0856 - val_loss: 1.9857 - val_mean_absolute_error: 1.1797\n",
      "Epoch 116/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0092 - mean_absolute_error: 0.0751 - val_loss: 2.0114 - val_mean_absolute_error: 1.1903\n",
      "Epoch 117/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0121 - mean_absolute_error: 0.0879 - val_loss: 2.0074 - val_mean_absolute_error: 1.1909\n",
      "Epoch 118/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0103 - mean_absolute_error: 0.0777 - val_loss: 1.9887 - val_mean_absolute_error: 1.1771\n",
      "Epoch 119/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0115 - mean_absolute_error: 0.0854 - val_loss: 1.9718 - val_mean_absolute_error: 1.1842\n",
      "Epoch 120/145\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.0128 - mean_absolute_error: 0.0887 - val_loss: 1.9740 - val_mean_absolute_error: 1.1782\n",
      "Epoch 121/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0104 - mean_absolute_error: 0.0763 - val_loss: 2.0167 - val_mean_absolute_error: 1.2047\n",
      "Epoch 122/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0132 - mean_absolute_error: 0.0915 - val_loss: 1.9849 - val_mean_absolute_error: 1.1830\n",
      "Epoch 123/145\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.0124 - mean_absolute_error: 0.0872 - val_loss: 1.9544 - val_mean_absolute_error: 1.1833\n",
      "Epoch 124/145\n",
      "800/800 [==============================] - 0s 155us/step - loss: 0.0128 - mean_absolute_error: 0.0891 - val_loss: 1.9774 - val_mean_absolute_error: 1.1856\n",
      "Epoch 125/145\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.0108 - mean_absolute_error: 0.0806 - val_loss: 1.9867 - val_mean_absolute_error: 1.1915\n",
      "Epoch 126/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0119 - mean_absolute_error: 0.0839 - val_loss: 1.9935 - val_mean_absolute_error: 1.1872\n",
      "Epoch 127/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0101 - mean_absolute_error: 0.0780 - val_loss: 2.0058 - val_mean_absolute_error: 1.1964\n",
      "Epoch 128/145\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.0079 - mean_absolute_error: 0.0695 - val_loss: 1.9785 - val_mean_absolute_error: 1.1788\n",
      "Epoch 129/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0091 - mean_absolute_error: 0.0757 - val_loss: 1.9773 - val_mean_absolute_error: 1.1878\n",
      "Epoch 130/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0088 - mean_absolute_error: 0.0677 - val_loss: 2.0046 - val_mean_absolute_error: 1.1783\n",
      "Epoch 131/145\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.0126 - mean_absolute_error: 0.0899 - val_loss: 1.9972 - val_mean_absolute_error: 1.2017\n",
      "Epoch 132/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0108 - mean_absolute_error: 0.0816 - val_loss: 1.9549 - val_mean_absolute_error: 1.1772\n",
      "Epoch 133/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0129 - mean_absolute_error: 0.0884 - val_loss: 1.9869 - val_mean_absolute_error: 1.1824\n",
      "Epoch 134/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0100 - mean_absolute_error: 0.0789 - val_loss: 2.0018 - val_mean_absolute_error: 1.1830\n",
      "Epoch 135/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0138 - mean_absolute_error: 0.0925 - val_loss: 2.0079 - val_mean_absolute_error: 1.1733\n",
      "Epoch 136/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0159 - mean_absolute_error: 0.1005 - val_loss: 2.0313 - val_mean_absolute_error: 1.2092\n",
      "Epoch 137/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0213 - mean_absolute_error: 0.1107 - val_loss: 1.9996 - val_mean_absolute_error: 1.1798\n",
      "Epoch 138/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0150 - mean_absolute_error: 0.0939 - val_loss: 1.9915 - val_mean_absolute_error: 1.1922\n",
      "Epoch 139/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0207 - mean_absolute_error: 0.1128 - val_loss: 1.9673 - val_mean_absolute_error: 1.1764\n",
      "Epoch 140/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0161 - mean_absolute_error: 0.0986 - val_loss: 1.9732 - val_mean_absolute_error: 1.1815\n",
      "Epoch 141/145\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.0232 - mean_absolute_error: 0.1212 - val_loss: 1.9969 - val_mean_absolute_error: 1.2052\n",
      "Epoch 142/145\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.0301 - mean_absolute_error: 0.1404 - val_loss: 1.9481 - val_mean_absolute_error: 1.1576\n",
      "Epoch 143/145\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0221 - mean_absolute_error: 0.1169 - val_loss: 1.9766 - val_mean_absolute_error: 1.1911\n",
      "Epoch 144/145\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0176 - mean_absolute_error: 0.1045 - val_loss: 1.9853 - val_mean_absolute_error: 1.1760\n",
      "Epoch 145/145\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.0186 - mean_absolute_error: 0.1092 - val_loss: 1.9812 - val_mean_absolute_error: 1.1971\n"
     ]
    }
   ],
   "source": [
    "model, loss = train_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon",
   "language": "python",
   "name": "datathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
